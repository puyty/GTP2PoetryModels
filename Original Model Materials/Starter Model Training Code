#gpt model on all manuscripts 5 epochs 
import nltk
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

# Set the paths and filenames
file_path = "/Users/sean/mu_code/trainingmaterials.txt"
output_dir = "/Users/sean/Downloads/gmodel1"

# Preprocess the corpus data
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Load and tokenize the corpus data from the file
with open(file_path, "r", encoding="utf-8") as file:
    corpus_data = file.read()
    tokens = nltk.word_tokenize(corpus_data.lower())
    input_ids = tokenizer.convert_tokens_to_ids(tokens)

# Create a dataset from the input IDs
dataset = TextDataset(
    tokenizer=tokenizer,
    file_path=file_path,
    block_size=128,
)

# Initialize the GPT-2 model
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Configure the training arguments
training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    num_train_epochs=5,
    per_device_train_batch_size=4,
    save_steps=500,
    save_total_limit=2,
    prediction_loss_only=True,
)

# Initialize the trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    train_dataset=dataset,
)

# Train the model
trainer.train()

# Save the model and tokenizer to the specified directory
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
